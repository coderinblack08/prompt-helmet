{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ollama import generate\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompts = pd.read_csv('datasets/system_prompts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 100\n",
    "user_inputs = []\n",
    "\n",
    "for _, row in system_prompts.iterrows():\n",
    "    system_prompt = row['system_prompt']\n",
    "    prompt_type = row['type']\n",
    "    \n",
    "    for i in range(NUM_EXAMPLES):\n",
    "        meta_prompt = f\"\"\"Based on this system prompt: \"{system_prompt}\"\n",
    "        Generate a realistic, brief user input (1-3 sentences) that someone might send to this assistant.\n",
    "        Only respond with the user input, nothing else.\"\"\"\n",
    "        \n",
    "        response = generate(model='llama3.1:8b', prompt=meta_prompt)\n",
    "        while not response.done:\n",
    "            continue\n",
    "        user_inputs.append(response.response)\n",
    "\n",
    "        print(f\"Generated {len(user_inputs)} user inputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_examples = []\n",
    "\n",
    "for i in range(1, len(system_prompts) + 1):\n",
    "    chunk = pd.DataFrame({\n",
    "        'user_input': user_inputs[(i-1)*NUM_EXAMPLES:i*NUM_EXAMPLES],\n",
    "        'system_prompt_id': [i] * NUM_EXAMPLES\n",
    "    })\n",
    "    chunked_examples.append(chunk)\n",
    "\n",
    "benign_examples = pd.concat(chunked_examples, ignore_index=True)\n",
    "benign_examples.to_csv('datasets/benign_examples.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"deepset/prompt-injections\")\n",
    "malicious_examples = pd.concat([\n",
    "    pd.DataFrame(dataset['train']).query('label == 1')[['text']],\n",
    "    pd.DataFrame(dataset['test']).query('label == 1')[['text']]\n",
    "])\n",
    "malicious_examples = malicious_examples.rename(columns={'text': 'user_input'})\n",
    "malicious_examples['system_prompt_id'] = np.random.randint(1, len(system_prompts) + 1, size=len(malicious_examples))\n",
    "malicious_examples.to_csv('datasets/malicious_examples.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
